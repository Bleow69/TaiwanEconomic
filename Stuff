##Pre-processing 

96 columns (Selecting relevant columns)
-Net income flag not needed (All 1 value)
-
Profitability Metrics:
-ROA(C), ROA(A), and ROA(B): Variants of Return on Assets are critical indicators of how efficiently assets generate income.
-Net Income to Total Assets: Measures how much profit the company earns relative to its assets.
-Operating Gross Margin, Operating Profit Rate: Assess operational efficiency and margins.

Liquidity Ratios:
-Current Ratio, Quick Ratio: Measure short-term financial health.
-Cash/Current Liability: Indicates how easily the company can cover liabilities with cash.

Leverage Ratios:
-Liability to Equity, Debt ratio %: Assess the companyâ€™s reliance on debt financing.
-Interest Coverage Ratio: Ability to service debt obligations.

Cash Flow Metrics:
-Cash Flow to Total Assets, CFO to Assets: Indicate cash-generating capacity.

Growth Rates:
-Total Asset Growth Rate, Net Value Growth Rate: Capture trends in financial health.

Expense Ratios:
-Total expense/Assets: Indicates cost efficiency relative to the asset base.

Remember to use feature selection to identify relevant predictors to avoid overfitting

Multi-class classfication is a type of classfication task where goal is to assign an instance/result to one or more possible clauses 
(e.g Red, Blue, Green, Black ball and you are trying to predict which ball)



While logistic regression can function without scaling for some datasets, feature scaling is a best practice, particularly when:
Features vary greatly in magnitude.
You use regularization.
You want faster and more stable convergence.

DO NOT NEED FEATURE SCALING/NORMALIZATION since they are already on the same scale
One-hot encoding is representing categorical variables as binary vector (e.g [0 1 0], [1 0 0]) 
Binary features - Values that can take on one or two values (usually True/False or Yes/No questions) 

Regularization
-Used to address overfitting or improve model generalization by penalizing large or unnecessary (feature) coefficients
-Adds penalty term to loss function, discourages model from fitting noise in training data 

Why it's needed?
1. Prevent overfitting
Caused by: 
-Dataset having more features relative to samples
-Highly correlated or redundant features
-Model allowed to assign large weights to specific features

2. Improve generalization 
A regularized model performs better on unseen data by avoiding fitting noise in training data

3. Handles multicollinearity 
(From point 1)

4. Feature selection:
L1 Regularization (Lasso): Sets coefficients to exactly zero 
L2 Regularization (Ridge): SHRINKS coefficents (but does not set them to 0), retaining all features 

How do you determine regularization constant?

1.Grid search with cross-validation
2.Randomized search
3.Bayesian Optimization
4.Validation curve (Plotting performance (e.g accuracy/F1 score) as a function of regularization constant)

Key considerations of Regularization constant
Small lamda - Less regularization --> Potential overfitting
Large lamda - Stronger regularization --> Potential underfitting 

For high-dimensional data(data where you have more features than observation/data points), stronger regularization is better 
For smaller datasets, weaker regularization is better 
(L1(LASSO) AND L2(RIDGE) may require different lamda values)

Evaluation
Confusion Matrix
True Positive (TP): predicted class matches the actual class; actual value was positive, model predicted positive
True Negative (TN): predicted class matches the actual class; actual value was negative, model predicted negative
False Positive (FP): predicted value was falsely predicted; actual value was negative; model predicted positive
False Negative (FN): predicted value was falsely predicted; actual value was positive; model predicted negative
Accuracy: shows how often a model is correct overall

Precision (Good measure when cost of False Positive high)
Measures how accurate model is out of predicted positive (how well model predicts the true positive) 

Recall (Good measure when cost of FALSE negative high)
Measures how many actual positives our model captures actual positive 

F1 (Helpful for uneven distributions; large number of actual negatives)
Balance between Precision and Recall
